# -*- coding: utf-8 -*-
"""Copy of Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14oyDL9EA9HzINnNicV3EHi4vJwfFoYcI

#Full Preprocessing & random forest classifier  Model Example
"""

# üöÄ Data Manipulation and Analysis
import pandas as pd  # DataFrame operations
import numpy as np   # Numerical computations

# üìä Data Visualization
import matplotlib.pyplot as plt  # Plotting static graphs
import seaborn as sns            # Advanced data visualization
import plotly.express as px      # Interactive visualizations
import plotly.graph_objects as go  # Custom interactive plots

# üß† Machine Learning and Evaluation
from sklearn.model_selection import train_test_split  # Dataset splitting
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder  # Feature scaling and encoding
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier  # Robust classification model
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix  # Evaluation metrics

# ‚è≥ Time Handling
from datetime import datetime  # Timestamp handling

# üõ† Utility Tools
import warnings
warnings.filterwarnings('ignore')  # Suppress warnings for cleaner outputs

# üåü Interactive Visualizations (Optional, install if needed)
try:
    import ipywidgets as widgets  # Interactive widgets for Jupyter notebooks
except ImportError:
    print("Optional library 'ipywidgets' not installed. Install via 'pip install ipywidgets' if required.")

# ‚úÖ Environment setup complete
print("Libraries successfully imported. Ready to proceed!")

#Load the dataset
from google.colab import files # Import the 'files' module from google.colab
uploaded = files.upload() # Use files.upload() to upload your file
data = pd.read_csv(next(iter(uploaded)))

# Display basic information about the dataset
print("\nüîç Dataset Info:")
print(data.info())

# Display the first few rows for a quick preview
print("\nüìã First 5 Rows:")
print(data.head())

# Display basic statistics for numerical columns
print("\nüìä Dataset Statistics:")
print(data.describe(include='all'))

# ===============================
# üîç Checking for Missing Data
# ===============================

# Display the count of missing values for each column
missing_data = data.isnull().sum()
print("\nüîç Missing Data Summary:")
print(missing_data[missing_data > 0])  # Display only columns with missing values

# Visualizing missing data (optional for better insight)
plt.figure(figsize=(6, 6))
sns.heatmap(data.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Data heatmap')
plt.show()

# Handling missing values (example: drop rows or fill missing data)
# Uncomment one of the following lines based on the project requirements
# data = data.dropna()  # Drop rows with missing values
# data.fillna(method='ffill', inplace=True)  # Fill missing values with forward fill

# ===============================
# ‚öñÔ∏è Homogeneity Testing
# ===============================

# Importing necessary libraries for statistical tests
from scipy.stats import levene

# Selecting numerical columns for homogeneity testing
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns

# Performing Levene's test for homogeneity
print("\n‚öñÔ∏è Homogeneity Test Results:")
for column in numerical_columns:
    stat, p_value = levene(data[column].dropna(), data[numerical_columns[0]].dropna())
    print(f"Column: {column} | Statistic: {stat:.3f} | P-value: {p_value:.3f}")

    if p_value < 0.05:
        print(f"‚ùå {column} does not have homogeneous variance.")
    else:
        print(f"‚úÖ {column} passes the homogeneity test.")

# Proceed with data normalization or feature engineering based on test results.
print("\n‚úÖ Missing data handling and homogeneity testing completed!")

# ‚öñÔ∏è Normalizing Features

# Columns with non-homogeneous variance
columns_to_normalize = ['bytes_out', 'response.code', 'dst_port']

# Applying log transformation to stabilize variance
for col in columns_to_normalize:
    data[col] = np.log1p(data[col])  # log1p handles log(0) safely

# üîß Scaling Features

# Importing MinMaxScaler for scaling
from sklearn.preprocessing import MinMaxScaler

# Initialize scaler
scaler = MinMaxScaler()

# Selecting numerical columns for scaling
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns

# Scaling the numerical columns
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# ‚úÖ Verification
# Display the first few rows of the normalized and scaled dataset
print("\nüìã Transformed Dataset (First 5 Rows):")
print(data.head())

# Check if the scaling was successful
print("\nüìä Summary Statistics After Scaling:")
print(data[numerical_columns].describe())

# üìå Next Step
# Proceed to feature engineering or model training.
print("\n‚úÖ Features normalized and scaled successfully!")

# üîç Dataset Overview
# Summary statistics for all columns
print("\nüìä Complete Summary Statistics:")
print(data.describe(include='all'))

# Visualizing distributions for numerical features
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns

for col in numerical_columns:
    plt.figure(figsize=(6, 6))
    sns.histplot(data[col], kde=True, bins=30, color='blue')
    plt.title(f'Distribution of({col}) ')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

# üìà Feature Correlation Analysis (Fixed)

# Selecting only numerical columns
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
correlation_matrix = data[numerical_columns].corr()

# Visualizing the correlation matrix
plt.figure(figsize=(8, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Feature Correlation Heatmap (Numerical Columns Only)')
plt.show()

# üîó Advanced Categorical Insights

categorical_columns = data.select_dtypes(include=['object']).columns

for col in categorical_columns:
    print(f"\nüóÇ Unique Values in {col}:")
    print(data[col].value_counts())

    plt.figure(figsize=(6, 6))
    sns.countplot(data=data, y=col, order=data[col].value_counts().index, palette="viridis")
    plt.title(f'Distribution of {col}')
    plt.xlabel('Count')
    plt.ylabel(col)
    plt.show()

# ‚ö†Ô∏è Advanced Anomaly Detection

# Using Z-score for outlier detection
from scipy.stats import zscore

for col in numerical_columns:
    z_scores = zscore(data[col])
    outliers = data[np.abs(z_scores) > 3]
    print(f"\n‚ö†Ô∏è Outliers detected in {col}: {len(outliers)}")

    # Visualizing outliers
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=data, x=col, color="red")
    plt.title(f'Outliers in {col}')
    plt.show()

# ‚ö†Ô∏è Advanced Outlier Detection

# Using IQR method for detailed outlier analysis
for col in numerical_columns:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    outliers = data[(data[col] < (Q1 - 1.5 * IQR)) | (data[col] > (Q3 + 1.5 * IQR))]
    print(f"\n‚ö†Ô∏è Outliers in {col}: {len(outliers)}")

    # Boxplot for visualizing outliers
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=data, x=col, color='red')
    plt.title(f'Outliers Analysis - {col}')
    plt.show()

# üìÖ Time-Based Trend Analysis

if 'time' in data.columns:
    data['time'] = pd.to_datetime(data['time'])

    plt.figure(figsize=(15, 6))
    sns.lineplot(data=data, x='time', y='bytes_in', label='Bytes In', color='blue')
    sns.lineplot(data=data, x='time', y='bytes_out', label='Bytes Out', color='orange')
    plt.title('Time-Based Trend Analysis')
    plt.xlabel('Time')
    plt.ylabel('Bytes')
    plt.legend()
    plt.show()

# ===============================
# ‚≠ê Feature Importance (Statistical)
# ===============================

# Using correlation to identify top features
top_features = correlation_matrix.abs().mean().sort_values(ascending=False).index[:5]
print("\n‚≠ê Top Features Based on Correlation:")
print(top_features)

# Visualizing the most important features
plt.figure(figsize=(12, 6))
sns.heatmap(correlation_matrix.loc[top_features, top_features], annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Top Feature Correlation Heatmap')
plt.show()

# ===============================
# üõ† Feature Creation
# ===============================

# Example: Creating a traffic intensity feature
data['traffic_intensity'] = data['bytes_in'] + data['bytes_out']
print("‚úÖ New feature 'traffic_intensity' created.")

# Example: Creating a duration feature if time columns exist
if 'creation_time' in data.columns and 'end_time' in data.columns:
    data['creation_time'] = pd.to_datetime(data['creation_time'])
    data['end_time'] = pd.to_datetime(data['end_time'])
    data['connection_duration'] = (data['end_time'] - data['creation_time']).dt.total_seconds()
    print("‚úÖ New feature 'connection_duration' created.")

# ===============================
# üîß Feature Transformation
# ===============================

# Applying log transformation to skewed features
skewed_features = ['traffic_intensity', 'bytes_in', 'bytes_out']
for col in skewed_features:
    data[f'{col}_log'] = np.log1p(data[col])
    print(f"‚úÖ Log-transformed feature '{col}_log' created.")

# ===============================
# üîó Feature Encoding
# ===============================

# Encoding categorical variables
categorical_columns = data.select_dtypes(include=['object']).columns
for col in categorical_columns:
    data[f'{col}_encoded'] = LabelEncoder().fit_transform(data[col])
    print(f"‚úÖ Categorical feature '{col}' encoded.")

# ===============================
# ‚≠ê Feature Selection (Fixed)
# ===============================

# Identifying features with low variance
from sklearn.feature_selection import VarianceThreshold

# Selecting numerical columns for variance thresholding
numerical_data = data.select_dtypes(include=['float64', 'int64'])

# Identifying features with low variance
low_variance_filter = VarianceThreshold(threshold=0.01)
low_variance_filter.fit(numerical_data)

# Mapping low variance features to their original dataset columns
low_variance_features = numerical_data.columns[~low_variance_filter.get_support()]
print(f"\n‚ö†Ô∏è Low variance features detected and removed: {list(low_variance_features)}")

# Dropping low variance features from the original dataset
data.drop(columns=low_variance_features, inplace=True, errors='ignore')

# Removing redundant features (e.g., original features replaced by transformed ones)
redundant_features = ['bytes_in', 'bytes_out', 'traffic_intensity']
data.drop(columns=redundant_features, inplace=True, errors='ignore')
print("‚úÖ Redundant features removed.")

# ===============================
# üìã Final Dataset Summary
# ===============================

print("\nüìã Final Dataset Features:")
print(data.columns)

print("\nüî¢ Final Dataset Shape:", data.shape)

# ===============================
# üîß Verification
# ===============================

print("\nüìã Final Dataset After Removing Low Variance Features:")
print(data.info())

# ===============================
# üîß Verification
# ===============================

print("\nüìã Final Dataset After Removing Low Variance Features:")
print(data.info())

# Define the target column
target_column = 'detection_types'  # Ensure this column exists in your dataset
if target_column not in data.columns:
    raise ValueError(f"‚ùå Target column '{target_column}' not found in the dataset.")

# Identify numerical and categorical columns
numerical_features = data.select_dtypes(include=['float64', 'int64']).columns.tolist()
categorical_features = data.select_dtypes(include=['object']).columns.tolist()

print(f"\nüî¢ Numerical Features: {numerical_features}")
print(f"üóÇÔ∏è Categorical Features: {categorical_features}")

# ===============================
# üìÇ Step 2: Splitting the Dataset
# ===============================
# Splitting features and target
X = data.drop(columns=[target_column], errors='ignore')  # Features
y = data[target_column]  # Target variable

# Splitting the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"\nüìä Dataset Split Completed:\n - Training set: {X_train.shape}\n - Testing set: {X_test.shape}")

# ===============================
# üìä Step 3: Preprocessing Pipelines
# ===============================
# Preprocessing for numerical features
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())  # Standard scaling for numerical features
])

# Preprocessing for categorical features
categorical_transformer = Pipeline(steps=[
    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # One-hot encoding for categorical features
])

# Combine preprocessors into a ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, [col for col in numerical_features if col in X.columns]),
        ('cat', categorical_transformer, [col for col in categorical_features if col in X.columns])
    ])

# üß† Step 4: Defining the Model Pipeline
# Model pipeline with preprocessing and classifier
model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42, n_estimators=100))
])

# üöÄ Step 5: Model Training
print("\nüöÄ Training Advanced Model Pipeline...")
model_pipeline.fit(X_train, y_train)

# Test set predictions
y_pred = model_pipeline.predict(X_test)

# ===============================
# üìä Step 6: Model Evaluation
# ===============================
print("\nüìã Classification Report:")
print(classification_report(y_test, y_pred))

accuracy = accuracy_score(y_test, y_pred)
print(f"‚úÖ Model Accuracy: {accuracy:.2f}")